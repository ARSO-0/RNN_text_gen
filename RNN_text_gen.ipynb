{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "u55ZFz29TkWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_file = tf.keras.utils.get_file('01-Genesis.org', 'https://github.com/bc-abe/plain-text-bibles/raw/master/AKJV-org/01-Genesis.org')\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('Bible.txt', 'https://github.com/ARSO-0/old_testament_sample_text/blob/main/Bible.txt?raw=true')\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it run on gpu\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "0R7BNk1VTwVq",
        "outputId": "579f5cfd-6fd4-410e-8cef-fcfa19ed3854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/ARSO-0/old_testament_sample_text/blob/main/Bible.txt?raw=true\n",
            "7373217/7373217 [==============================] - 0s 0us/step\n",
            "Length of text: 4164321 characters\n",
            "89 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "\n",
        "#Create training batches\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "#Build The Model\n",
        "\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UPIfpVCkTwuS",
        "outputId": "7453bafa-8065-445b-a197-40433b8c3e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n",
            "(64, 100, 90) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  23040     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  92250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,053,594\n",
            "Trainable params: 4,053,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "history = model.fit(dataset, epochs=20, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "BWFa3BTGTwyi",
        "outputId": "0b44384e-e01e-4bee-fecf-8ca24a22c4b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "644/644 [==============================] - 35s 51ms/step - loss: 2.1617\n",
            "Epoch 2/20\n",
            "644/644 [==============================] - 36s 54ms/step - loss: 1.4890\n",
            "Epoch 3/20\n",
            "644/644 [==============================] - 36s 54ms/step - loss: 1.3402\n",
            "Epoch 4/20\n",
            "644/644 [==============================] - 37s 55ms/step - loss: 1.2654\n",
            "Epoch 5/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.2136\n",
            "Epoch 6/20\n",
            "644/644 [==============================] - 38s 57ms/step - loss: 1.1715\n",
            "Epoch 7/20\n",
            "644/644 [==============================] - 37s 55ms/step - loss: 1.1349\n",
            "Epoch 8/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.1030\n",
            "Epoch 9/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.0734\n",
            "Epoch 10/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.0476\n",
            "Epoch 11/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.0248\n",
            "Epoch 12/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 1.0052\n",
            "Epoch 13/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9888\n",
            "Epoch 14/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9753\n",
            "Epoch 15/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9639\n",
            "Epoch 16/20\n",
            "644/644 [==============================] - 38s 57ms/step - loss: 0.9558\n",
            "Epoch 17/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9489\n",
            "Epoch 18/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9436\n",
            "Epoch 19/20\n",
            "644/644 [==============================] - 38s 57ms/step - loss: 0.9405\n",
            "Epoch 20/20\n",
            "644/644 [==============================] - 37s 56ms/step - loss: 0.9381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate text\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state.\n",
        "The model returns a prediction for the next character and its new state.\n",
        "Pass the prediction and state back in to continue generating text.\n",
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "CUVvMOK5W2Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Бытие'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "metadata": {
        "id": "XDzPCmZjWaaa",
        "outputId": "1ac035bf-1e86-41ad-f4e3-6c7c2281c950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бытие: вень около трех дней во, даре пророка, Господа и Богом Рагава и Аарона и сыновья их, и сыновей в Иарине и в Шамафе, и вас; вы научились смиренномудрием огорченными людьми:\n",
            "14 нужно истребление сходов приновятся в землю, которую Я даю тебе.\n",
            "10 Смотри по жребии моем, который приноси Емугу свою и проклят тестью, собирается седому, сказав: \"зайта ваши Царствия Божия: \"разрушит слово, которое уже не будешь взять храм в комнатах твоих\".\n",
            "16 Итак похорони меча, но авно пройдет гонцев и прелюбодействие, согрешав, пока остается над нею во всесожжение Господу Богу вашему;\n",
            "6 как же спасало предопределение, некому духовным дать от страны севира; подая им вперед.\n",
            "16 Видите ли, Господи! что мне слово ко мне: не тотчас отнято от Отца во дни его с Иисусом, и оберет Иезении, сын Карея, и Сидону и братьям\n",
            "2 и с ним в Иудее, воскликнул в своем [обещанию; постоянно оставался этим своим\n",
            "28 и начал говорит им Иисус: разве нет там, как делал царь царство в внешнем доме, иду к Тебе, и назначил дел Его.\n",
            "3 Ибо \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.140683174133301\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}